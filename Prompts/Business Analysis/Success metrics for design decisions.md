---
title: Success metrics for design decisions
category: Business Analysis
tags:
  - pm
  - success-metrics
  - business-analysis
  - design
---
INPUTS
<provided_inputs>
- {{DESIGN_GOALS}}
- {{PRODUCT_CONTEXT}}
- {{USER_OUTCOMES}}
</provided_inputs>

GOAL
Translate `DESIGN_GOALS`, `PRODUCT_CONTEXT`, and `USER_OUTCOMES` into a measurable success-metrics system for design decisions.
Success metric:
- Defines success from user, business, and product perspectives.
- Produces primary/secondary/guardrail/leading metrics with SMART definitions.
- Includes instrumentation, baseline/target logic, and decision-ready analysis plan.
- Follows the required output structure exactly.

CONSTRAINTS
- Use only provided inputs and clearly state assumptions when information is missing.
- Do not skip required metric-design steps:
  1. Define success (user/business/product).
  2. Map metric types (behavior, outcome, business, quality, leading).
  3. Apply SMART criteria to each key metric.
  4. Define measurement approach (instrumentation, tools, sample/stat criteria).
  5. Set baselines and targets (minimum, target, stretch, timeframe).
  6. Add guardrails for unintended consequences.
- Keep metrics decision-oriented and linked to design goals (avoid vanity metrics).
- Explicitly note risks/limitations where metrics may not capture full reality.
- Label assumptions clearly when baseline/tooling/sample data is incomplete.

FORMAT
Return exactly this structure:

<success_metrics_framework>
<success_definitions>
**User Success Means:**
[Describe what improves from user perspective - concrete outcomes]

**Business Success Means:**
[Describe what improves from business perspective - concrete outcomes]

**Product Success Means:**
[Describe what improves in the product - concrete outcomes]
</success_definitions>

<primary_metrics>
<metric_1>
**Metric Name:** [Clear, specific metric name]

**What It Measures:** [Exactly what behavior or outcome]

**Why It Matters:** [How it connects to goals]

**Type:** [Behavior/Outcome/Business/Quality]

**Measurement Method:**
- How: [Specific tracking approach]
- Where: [What system/tool]
- Frequency: [How often measured]
- Sample: [Who/what is included]

**Current Baseline:** [If known, current performance]

**Targets:**
- Minimum acceptable: [X%/number]
- Target: [Y%/number]
- Stretch: [Z%/number]
- Timeframe: [When to measure]

**Statistical Criteria:**
- Sample size needed: [N users/sessions]
- Significance level: [e.g., 95% confidence]
- Minimum detectable effect: [smallest meaningful change]

**Risks/Limitations:**
[What could make this metric misleading? What doesn't it capture?]
</metric_1>

<metric_2>
[Repeat structure for 3-5 primary metrics]
</metric_2>
</primary_metrics>

<supporting_metrics>
[List 3-5 secondary metrics that provide additional context:
- Metric name: [Description, why it's useful]
- Metric name: [Description, why it's useful]]
</supporting_metrics>

<guardrail_metrics>
[Metrics to ensure you're not causing harm:
- Metric: [Description]
- Threshold: [What value would indicate a problem]
- Why: [What unintended consequence this guards against]]
</guardrail_metrics>

<leading_indicators>
[Metrics that predict success before primary metrics show results:
- Indicator: [Description]
- Why it predicts success: [Connection to outcomes]
- When to measure: [Timeline]]
</leading_indicators>

<measurement_plan>
**Implementation Requirements:**
- Events to track: [List specific events]
- Properties to capture: [Data points per event]
- Tools needed: [Analytics platform, A/B test framework, survey tool, etc.]
- Team dependencies: [Who needs to implement]
- Timeline: [When instrumentation will be ready]

**Analysis Plan:**
- Segments to analyze: [User cohorts, use cases, etc.]
- Comparison approach: [A/B test, before/after, cohort comparison]
- Reporting cadence: [Daily/Weekly/Monthly]
- Decision timeline: [When to make go/no-go decision]

**Baseline Collection:**
[If redesign, describe how to establish current state before changes]
</measurement_plan>

<tradeoff_framework>
[How to make decisions when metrics conflict:
- If [Metric A] improves but [Metric B] declines, prioritize [A/B] because [rationale]
- Acceptable tradeoffs: [What you're willing to sacrifice for what gain]
- Unacceptable tradeoffs: [What must never decline]]
</tradeoff_framework>

<qualitative_validation>
[How to supplement quantitative metrics:
- User interviews: [What to ask]
- Usability testing: [What to observe]
- Support tickets: [What patterns to look for]
- Feedback surveys: [What questions to include]]
</qualitative_validation>

<one_page_dashboard>
[Describe what a single-page success dashboard should show:
- Key metric tiles
- Trend visualization
- Segment breakdowns
- Guardrail status
- Action triggers]
</one_page_dashboard>
</success_metrics_framework>

FAILURE
- Any required schema section is missing or malformed.
- Metrics are not SMART or not clearly tied to design goals/outcomes.
- Baselines/targets/timeframes are missing for primary metrics.
- Guardrail or leading indicators are missing.
- Measurement plan lacks instrumentation detail or decision cadence.
- Claims are generic or not grounded in provided inputs.
- Assumptions are used but not explicitly stated.
